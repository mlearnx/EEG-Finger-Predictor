{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM & CNN Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc_bci as bci\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['font.size'] = 28\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "c = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plt.rcParams['figure.figsize'] = 8, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(train_losses, val_losses, train_accs, val_accs):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(train_losses, '--', c=c[0], label='Train loss')\n",
    "    plt.plot(val_losses, c=c[0], label='Val loss')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(train_accs, '--', c=c[1], label='Train acc')\n",
    "    plt.plot(val_accs, c=c[1], label='Val acc')\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Loading and transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 500])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target = bci.load(root = './data_bci', train=True, one_khz=True)\n",
    "print(str(type(train_input)), train_input.size())\n",
    "print(str(type(train_target)), train_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 500])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "test_input, test_target = bci.load(root = './data_bci', train=False, one_khz=True)\n",
    "print(str(type(test_input)), test_input.size())\n",
    "print(str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([316, 28, 500])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_input.permute(0,2,1)\n",
    "test_input = test_input.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([316, 500, 28]), torch.Size([100, 500, 28]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape, test_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(train_input, train_target, std_dev, multiplier):\n",
    "    new_train_input = train_input.clone()\n",
    "    new_train_target = train_target.clone()\n",
    "    for i in range(multiplier-1):\n",
    "        augmented_input = train_input + torch.zeros(train_input.shape).normal_(0, std_dev)\n",
    "        new_train_input = torch.cat((new_train_input, augmented_input))\n",
    "        new_train_target = torch.cat((new_train_target, train_target))\n",
    "    return new_train_input, new_train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = augment_dataset(train_input, train_target, 0.1, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating the Dataset / Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_input, train_target)\n",
    "test_dataset = TensorDataset(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "dset_loaders = {'train': trainloader, 'val': testloader}\n",
    "dset_sizes = {'train': len(train_input), 'val': len(test_input)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Defining the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([316, 500, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq = train_input.shape[1] # 500\n",
    "#input_size = train_input.shape[2] #28\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_time_steps, n_features, batch_size, lstm_hidden_size=32, \n",
    "                 lstm_layers=1, dropout=0.2, conv3_layers=0, conv5_layers=0, \n",
    "                 conv7_layers=0, conv_channels=1, fc_layers=0, fc_size=256):\n",
    "        super(Net, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.conv3_layers = conv3_layers\n",
    "        self.conv5_layers = conv5_layers\n",
    "        self.conv7_layers = conv7_layers\n",
    "        self.fc_layers = fc_layers\n",
    "        self.concat_size = 0\n",
    "        \n",
    "        # LSTM input size is: (batch_size, n_time_steps, n_features)\n",
    "        # LSTM output size is: (batch_size, n_time_steps, lstm_hidden_size)\n",
    "        if lstm_layers:\n",
    "            self.concat_size += lstm_hidden_size\n",
    "            self.lstm = torch.nn.LSTM(input_size=n_features,\n",
    "                                      hidden_size=lstm_hidden_size,\n",
    "                                      num_layers=lstm_layers,\n",
    "                                      batch_first=True,\n",
    "                                      dropout=dropout)\n",
    "        \n",
    "        # Conv input size is: (batch_size, n_features, n_time_steps)\n",
    "        # Conv output size is: (batch_size, conv_channels or 1, n_time_steps)\n",
    "        if conv3_layers:\n",
    "            self.concat_size += n_time_steps\n",
    "            self.conv3a = torch.nn.Conv1d(in_channels=n_features,\n",
    "                                          out_channels=conv_channels,\n",
    "                                          kernel_size=3,\n",
    "                                          padding=1)\n",
    "        if conv3_layers > 1:\n",
    "            self.conv3b = torch.nn.Conv1d(in_channels=conv_channels,\n",
    "                                          out_channels=1,\n",
    "                                          kernel_size=3,\n",
    "                                          padding=1)\n",
    "        \n",
    "        if conv5_layers:\n",
    "            self.concat_size += n_time_steps\n",
    "            self.conv5a = torch.nn.Conv1d(in_channels=n_features,\n",
    "                                          out_channels=conv_channels,\n",
    "                                          kernel_size=5,\n",
    "                                          padding=2)\n",
    "        if conv5_layers > 1:\n",
    "            self.conv5b = torch.nn.Conv1d(in_channels=conv_channels,\n",
    "                                          out_channels=1,\n",
    "                                          kernel_size=5,\n",
    "                                          padding=2)\n",
    "        \n",
    "        if conv7_layers:\n",
    "            self.concat_size += n_time_steps\n",
    "            self.conv7a = torch.nn.Conv1d(in_channels=n_features,\n",
    "                                          out_channels=conv_channels,\n",
    "                                          kernel_size=7,\n",
    "                                          padding=3)\n",
    "        if conv7_layers > 1:\n",
    "            self.conv7b = torch.nn.Conv1d(in_channels=conv_channels,\n",
    "                                          out_channels=1,\n",
    "                                          kernel_size=7,\n",
    "                                          padding=3)\n",
    "        \n",
    "        if fc_layers:\n",
    "            self.fca = torch.nn.Linear(self.concat_size, n_hidden)\n",
    "            self.fcb = torch.nn.Linear(n_hidden, 2)\n",
    "        else:\n",
    "            self.fcb = torch.nn.Linear(self.concat_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        concat_list = []\n",
    "        \n",
    "        if self.lstm_layers:\n",
    "            lstm_out = self.lstm(x)[0][:,-1,:] # take only last output of LSTM (many-to-one RNN)\n",
    "            lstm_out = lstm_out.view(lstm_out.shape[0], -1) # flatten to (batch, lstm_hidden_size)\n",
    "            concat_list.append(lstm_out)\n",
    "            \n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        if self.conv3_layers:\n",
    "            c3 = F.relu(self.conv3a(x))\n",
    "            for i in range(self.conv3_layers - 1):\n",
    "                c3 = F.relu(self.conv3b(c3))\n",
    "            c3 = c3.view(c3.shape[0], -1)\n",
    "            concat_list.append(c3)\n",
    "            \n",
    "        if self.conv5_layers:\n",
    "            c5 = F.relu(self.conv5a(x))\n",
    "            for i in range(self.conv5_layers - 1):\n",
    "                c5 = F.relu(self.conv5b(c5))\n",
    "            c5 = c5.view(c5.shape[0], -1)\n",
    "            concat_list.append(c5)\n",
    "            \n",
    "        if self.conv7_layers:\n",
    "            c7 = F.relu(self.conv7a(x))\n",
    "            for i in range(self.conv7_layers - 1):\n",
    "                c7 = F.relu(self.conv7b(c7))\n",
    "            c7 = c7.view(c7.shape[0], -1)\n",
    "            concat_list.append(c7)\n",
    "        \n",
    "        out = torch.cat(concat_list, 1)\n",
    "        \n",
    "        for i in range(self.fc_layers):\n",
    "            out = self.fca(out)\n",
    "        out = self.fcb(out)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialising the hidden layer\n",
    "        return (Variable(torch.zeros(self.batch_size, self.lstm_layers, self.lstm_hidden_size)),\n",
    "                Variable(torch.zeros(self.batch_size, self.lstm_layers, self.lstm_hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    1     2     3     1     2     3     1     2     3\n",
       "    4     5     6     4     5     6     4     5     6\n",
       "[torch.FloatTensor of size 2x9]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf = Variable(torch.Tensor([[1,2,3],[4,5,6]]))\n",
    "\n",
    "torch.cat([asdf, asdf, asdf], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.cuda.FloatTensor of size 4 (GPU 0)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss = Variable(torch.zeros((4,0))).cuda()\n",
    "sss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dset_loaders, dset_sizes, criterion, optimizer,\\\n",
    "                lr_scheduler=None, num_epochs=25, verbose=2):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose > 1:\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                if lr_scheduler:\n",
    "                    optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dset_loaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs, labels = Variable(inputs.cuda()), \\\n",
    "                        Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                #_, preds = torch.max(outputs.data, 1)\n",
    "                preds = outputs\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                preds_classes = preds.data.max(1)[1]\n",
    "                running_corrects += torch.sum(preds_classes == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "                train_accs.append(epoch_acc)\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)\n",
    "                val_accs.append(epoch_acc)\n",
    "\n",
    "            if verbose > 1:\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "        if verbose > 1: \n",
    "            print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    if verbose > 0:\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    return best_model, train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(model, train_input, train_target, criterion, optimizer, lr_scheduler=None,\\\n",
    "              num_epochs=25, K=5, shuffle=True, random_seed=42, verbose=2,\n",
    "              augment_multiplier=0, std_dev=0.1):\n",
    "    n_train = len(train_input)\n",
    "    indices = list(range(n_train))\n",
    "    n_validation = n_train // K\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    avg_train_loss, avg_val_loss = 0, 0\n",
    "    avg_train_acc, avg_val_acc = 0, 0\n",
    "        \n",
    "    for k in range(K):\n",
    "        indices_rolled = np.roll(indices, k * n_train // K)\n",
    "        train_idx, val_idx = indices_rolled[n_validation:], indices_rolled[:n_validation]\n",
    "        \n",
    "        train_inp = train_input[train_idx,]\n",
    "        train_tar = train_target[train_idx,]\n",
    "        val_inp = train_input[val_idx,]\n",
    "        val_tar = train_target[val_idx,]\n",
    "        \n",
    "        train_inp, train_tar = augment_dataset(train_inp, train_tar, std_dev, augment_multiplier)\n",
    "        \n",
    "        #train_sampler = SubsetRandomSampler(train_idx) #sampler=train_sampler\n",
    "        #val_sampler = SubsetRandomSampler(val_idx) #sampler=val_sampler\n",
    "        \n",
    "        train_dataset = TensorDataset(train_inp, train_tar)\n",
    "        val_dataset = TensorDataset(val_inp, val_tar)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "        dset_loaders = {'train': train_loader, 'val': val_loader}\n",
    "        dset_sizes = {'train': len(train_inp), 'val': len(val_inp)}\n",
    "        \n",
    "        if verbose:\n",
    "            print('CV k={}:'.format(k))\n",
    "        _, train_losses, val_losses, train_accs, val_accs = train_model(copy.deepcopy(model), dset_loaders, \n",
    "                                                                        dset_sizes, criterion, optimizer, \n",
    "                                                                        num_epochs=num_epochs, verbose=verbose)\n",
    "        \n",
    "        avg_train_loss += min(train_losses)\n",
    "        avg_val_loss += min(val_losses)\n",
    "        avg_train_acc += max(train_accs)\n",
    "        avg_val_acc += max(val_accs)\n",
    "        \n",
    "    avg_train_loss /= K\n",
    "    avg_val_loss /= K\n",
    "    avg_train_acc /= K\n",
    "    avg_val_acc /= K\n",
    "        \n",
    "    if verbose:\n",
    "        print('\\nAvg best train loss: {:.2f}, avg best val loss: {:.2f}'.format(avg_train_loss, avg_val_loss))\n",
    "        print('Avg best train acc: {:.6f}%, avg best val acc: {:.6f}%'.format(avg_train_acc*100, avg_val_acc*100))\n",
    "        \n",
    "    return avg_train_loss, avg_val_loss, avg_train_acc, avg_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test blabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(train_input.shape[1], train_input.shape[2], batch_size, lstm_hidden_size=32, \n",
    "            lstm_layers=1, dropout=0.2, conv3_layers=2, conv5_layers=0, \n",
    "            conv7_layers=0, conv_channels=32, fc_layers=0, fc_size=256)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-1 # L2 regularizer parameter\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/249\n",
      "----------\n",
      "train Loss: 0.0211 Acc: 0.5618\n",
      "val Loss: 0.0279 Acc: 0.5400\n",
      "\n",
      "Epoch 1/249\n",
      "----------\n",
      "train Loss: 0.0194 Acc: 0.7016\n",
      "val Loss: 0.0310 Acc: 0.5700\n",
      "\n",
      "Epoch 2/249\n",
      "----------\n",
      "train Loss: 0.0170 Acc: 0.7611\n",
      "val Loss: 0.0381 Acc: 0.5300\n",
      "\n",
      "Epoch 3/249\n",
      "----------\n",
      "train Loss: 0.0146 Acc: 0.7985\n",
      "val Loss: 0.0448 Acc: 0.5700\n",
      "\n",
      "Epoch 4/249\n",
      "----------\n",
      "train Loss: 0.0125 Acc: 0.8392\n",
      "val Loss: 0.0499 Acc: 0.6200\n",
      "\n",
      "Epoch 5/249\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.8790\n",
      "val Loss: 0.0525 Acc: 0.6400\n",
      "\n",
      "Epoch 6/249\n",
      "----------\n",
      "train Loss: 0.0092 Acc: 0.9050\n",
      "val Loss: 0.0560 Acc: 0.6200\n",
      "\n",
      "Epoch 7/249\n",
      "----------\n",
      "train Loss: 0.0083 Acc: 0.9140\n",
      "val Loss: 0.0608 Acc: 0.6200\n",
      "\n",
      "Epoch 8/249\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.9223\n",
      "val Loss: 0.0624 Acc: 0.6600\n",
      "\n",
      "Epoch 9/249\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.9445\n",
      "val Loss: 0.0597 Acc: 0.6800\n",
      "\n",
      "Epoch 10/249\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.9536\n",
      "val Loss: 0.0632 Acc: 0.6500\n",
      "\n",
      "Epoch 11/249\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.9642\n",
      "val Loss: 0.0632 Acc: 0.6700\n",
      "\n",
      "Epoch 12/249\n",
      "----------\n",
      "train Loss: 0.0048 Acc: 0.9656\n",
      "val Loss: 0.0631 Acc: 0.6500\n",
      "\n",
      "Epoch 13/249\n",
      "----------\n",
      "train Loss: 0.0042 Acc: 0.9694\n",
      "val Loss: 0.0628 Acc: 0.6700\n",
      "\n",
      "Epoch 14/249\n",
      "----------\n",
      "train Loss: 0.0039 Acc: 0.9720\n",
      "val Loss: 0.0641 Acc: 0.6600\n",
      "\n",
      "Epoch 15/249\n",
      "----------\n",
      "train Loss: 0.0040 Acc: 0.9742\n",
      "val Loss: 0.0644 Acc: 0.6900\n",
      "\n",
      "Epoch 16/249\n",
      "----------\n",
      "train Loss: 0.0036 Acc: 0.9775\n",
      "val Loss: 0.0628 Acc: 0.7200\n",
      "\n",
      "Epoch 17/249\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.9862\n",
      "val Loss: 0.0623 Acc: 0.6800\n",
      "\n",
      "Epoch 18/249\n",
      "----------\n",
      "train Loss: 0.0036 Acc: 0.9778\n",
      "val Loss: 0.0610 Acc: 0.6800\n",
      "\n",
      "Epoch 19/249\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 0.9866\n",
      "val Loss: 0.0615 Acc: 0.6800\n",
      "\n",
      "Epoch 20/249\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 0.9868\n",
      "val Loss: 0.0591 Acc: 0.7000\n",
      "\n",
      "Epoch 21/249\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 0.9852\n",
      "val Loss: 0.0622 Acc: 0.6600\n",
      "\n",
      "Epoch 22/249\n",
      "----------\n",
      "train Loss: 0.0031 Acc: 0.9850\n",
      "val Loss: 0.0628 Acc: 0.6500\n",
      "\n",
      "Epoch 23/249\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 0.9862\n",
      "val Loss: 0.0603 Acc: 0.6700\n",
      "\n",
      "Epoch 24/249\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 0.9857\n",
      "val Loss: 0.0598 Acc: 0.6400\n",
      "\n",
      "Epoch 25/249\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9905\n",
      "val Loss: 0.0620 Acc: 0.6600\n",
      "\n",
      "Epoch 26/249\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.9908\n",
      "val Loss: 0.0583 Acc: 0.6900\n",
      "\n",
      "Epoch 27/249\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9914\n",
      "val Loss: 0.0604 Acc: 0.6600\n",
      "\n",
      "Epoch 28/249\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.9911\n",
      "val Loss: 0.0579 Acc: 0.6700\n",
      "\n",
      "Epoch 29/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9915\n",
      "val Loss: 0.0558 Acc: 0.7100\n",
      "\n",
      "Epoch 30/249\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9917\n",
      "val Loss: 0.0556 Acc: 0.7100\n",
      "\n",
      "Epoch 31/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9932\n",
      "val Loss: 0.0576 Acc: 0.7300\n",
      "\n",
      "Epoch 32/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9920\n",
      "val Loss: 0.0558 Acc: 0.6500\n",
      "\n",
      "Epoch 33/249\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9948\n",
      "val Loss: 0.0554 Acc: 0.6600\n",
      "\n",
      "Epoch 34/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9921\n",
      "val Loss: 0.0560 Acc: 0.6800\n",
      "\n",
      "Epoch 35/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9953\n",
      "val Loss: 0.0568 Acc: 0.6700\n",
      "\n",
      "Epoch 36/249\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9890\n",
      "val Loss: 0.0567 Acc: 0.6800\n",
      "\n",
      "Epoch 37/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9928\n",
      "val Loss: 0.0533 Acc: 0.6800\n",
      "\n",
      "Epoch 38/249\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.9905\n",
      "val Loss: 0.0563 Acc: 0.7000\n",
      "\n",
      "Epoch 39/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9931\n",
      "val Loss: 0.0553 Acc: 0.6800\n",
      "\n",
      "Epoch 40/249\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9941\n",
      "val Loss: 0.0557 Acc: 0.6800\n",
      "\n",
      "Epoch 41/249\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9935\n",
      "val Loss: 0.0541 Acc: 0.6900\n",
      "\n",
      "Epoch 42/249\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9880\n",
      "val Loss: 0.0534 Acc: 0.6900\n",
      "\n",
      "Epoch 43/249\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9932\n",
      "val Loss: 0.0549 Acc: 0.7100\n",
      "\n",
      "Epoch 44/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9930\n",
      "val Loss: 0.0551 Acc: 0.7000\n",
      "\n",
      "Epoch 45/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9924\n",
      "val Loss: 0.0544 Acc: 0.6800\n",
      "\n",
      "Epoch 46/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9934\n",
      "val Loss: 0.0488 Acc: 0.7100\n",
      "\n",
      "Epoch 47/249\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 0.9886\n",
      "val Loss: 0.0498 Acc: 0.6900\n",
      "\n",
      "Epoch 48/249\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9925\n",
      "val Loss: 0.0492 Acc: 0.6800\n",
      "\n",
      "Epoch 49/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9917\n",
      "val Loss: 0.0526 Acc: 0.6900\n",
      "\n",
      "Epoch 50/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9953\n",
      "val Loss: 0.0516 Acc: 0.7000\n",
      "\n",
      "Epoch 51/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9944\n",
      "val Loss: 0.0538 Acc: 0.7000\n",
      "\n",
      "Epoch 52/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9921\n",
      "val Loss: 0.0479 Acc: 0.6800\n",
      "\n",
      "Epoch 53/249\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9945\n",
      "val Loss: 0.0489 Acc: 0.7000\n",
      "\n",
      "Epoch 54/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9948\n",
      "val Loss: 0.0499 Acc: 0.6900\n",
      "\n",
      "Epoch 55/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9908\n",
      "val Loss: 0.0490 Acc: 0.7000\n",
      "\n",
      "Epoch 56/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9930\n",
      "val Loss: 0.0484 Acc: 0.7200\n",
      "\n",
      "Epoch 57/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9925\n",
      "val Loss: 0.0516 Acc: 0.7300\n",
      "\n",
      "Epoch 58/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9951\n",
      "val Loss: 0.0479 Acc: 0.7000\n",
      "\n",
      "Epoch 59/249\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9891\n",
      "val Loss: 0.0555 Acc: 0.7000\n",
      "\n",
      "Epoch 60/249\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.9914\n",
      "val Loss: 0.0573 Acc: 0.6900\n",
      "\n",
      "Epoch 61/249\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 0.9845\n",
      "val Loss: 0.0513 Acc: 0.6900\n",
      "\n",
      "Epoch 62/249\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9925\n",
      "val Loss: 0.0519 Acc: 0.6900\n",
      "\n",
      "Epoch 63/249\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9921\n",
      "val Loss: 0.0492 Acc: 0.7100\n",
      "\n",
      "Epoch 64/249\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9843\n",
      "val Loss: 0.0508 Acc: 0.7000\n",
      "\n",
      "Epoch 65/249\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.9892\n",
      "val Loss: 0.0516 Acc: 0.7000\n",
      "\n",
      "Epoch 66/249\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9910\n",
      "val Loss: 0.0433 Acc: 0.7000\n",
      "\n",
      "Epoch 67/249\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 0.9908\n",
      "val Loss: 0.0466 Acc: 0.6900\n",
      "\n",
      "Epoch 68/249\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9929\n",
      "val Loss: 0.0473 Acc: 0.6800\n",
      "\n",
      "Epoch 69/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9937\n",
      "val Loss: 0.0487 Acc: 0.7100\n",
      "\n",
      "Epoch 70/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9937\n",
      "val Loss: 0.0484 Acc: 0.7200\n",
      "\n",
      "Epoch 71/249\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9854\n",
      "val Loss: 0.0500 Acc: 0.7300\n",
      "\n",
      "Epoch 72/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9940\n",
      "val Loss: 0.0498 Acc: 0.7000\n",
      "\n",
      "Epoch 73/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9891\n",
      "val Loss: 0.0495 Acc: 0.6600\n",
      "\n",
      "Epoch 74/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9936\n",
      "val Loss: 0.0475 Acc: 0.7000\n",
      "\n",
      "Epoch 75/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9935\n",
      "val Loss: 0.0510 Acc: 0.6700\n",
      "\n",
      "Epoch 76/249\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 0.9859\n",
      "val Loss: 0.0527 Acc: 0.7000\n",
      "\n",
      "Epoch 77/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9929\n",
      "val Loss: 0.0521 Acc: 0.7000\n",
      "\n",
      "Epoch 78/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9941\n",
      "val Loss: 0.0516 Acc: 0.7000\n",
      "\n",
      "Epoch 79/249\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9926\n",
      "val Loss: 0.0515 Acc: 0.7000\n",
      "\n",
      "Epoch 80/249\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9888\n",
      "val Loss: 0.0565 Acc: 0.6600\n",
      "\n",
      "Epoch 81/249\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9844\n",
      "val Loss: 0.0483 Acc: 0.7000\n",
      "\n",
      "Epoch 82/249\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9937\n",
      "val Loss: 0.0480 Acc: 0.7200\n",
      "\n",
      "Epoch 83/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9908\n",
      "val Loss: 0.0523 Acc: 0.6800\n",
      "\n",
      "Epoch 84/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9924\n",
      "val Loss: 0.0493 Acc: 0.6600\n",
      "\n",
      "Epoch 85/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9950\n",
      "val Loss: 0.0483 Acc: 0.7200\n",
      "\n",
      "Epoch 86/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9896\n",
      "val Loss: 0.0491 Acc: 0.6500\n",
      "\n",
      "Epoch 87/249\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9861\n",
      "val Loss: 0.0498 Acc: 0.6800\n",
      "\n",
      "Epoch 88/249\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 0.9869\n",
      "val Loss: 0.0507 Acc: 0.6600\n",
      "\n",
      "Epoch 89/249\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9893\n",
      "val Loss: 0.0477 Acc: 0.6800\n",
      "\n",
      "Epoch 90/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9954\n",
      "val Loss: 0.0487 Acc: 0.6800\n",
      "\n",
      "Epoch 91/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9934\n",
      "val Loss: 0.0488 Acc: 0.6800\n",
      "\n",
      "Epoch 92/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9948\n",
      "val Loss: 0.0502 Acc: 0.7100\n",
      "\n",
      "Epoch 93/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9896\n",
      "val Loss: 0.0491 Acc: 0.6900\n",
      "\n",
      "Epoch 94/249\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9934\n",
      "val Loss: 0.0482 Acc: 0.7000\n",
      "\n",
      "Epoch 95/249\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9923\n",
      "val Loss: 0.0461 Acc: 0.7100\n",
      "\n",
      "Epoch 96/249\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 0.9867\n",
      "val Loss: 0.0440 Acc: 0.6700\n",
      "\n",
      "Epoch 97/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9966\n",
      "val Loss: 0.0456 Acc: 0.6700\n",
      "\n",
      "Epoch 98/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9968\n",
      "val Loss: 0.0447 Acc: 0.6800\n",
      "\n",
      "Epoch 99/249\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9893\n",
      "val Loss: 0.0452 Acc: 0.7000\n",
      "\n",
      "Epoch 100/249\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9847\n",
      "val Loss: 0.0495 Acc: 0.6600\n",
      "\n",
      "Epoch 101/249\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9852\n",
      "val Loss: 0.0500 Acc: 0.6400\n",
      "\n",
      "Epoch 102/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9961\n",
      "val Loss: 0.0477 Acc: 0.6700\n",
      "\n",
      "Epoch 103/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9968\n",
      "val Loss: 0.0467 Acc: 0.6700\n",
      "\n",
      "Epoch 104/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9966\n",
      "val Loss: 0.0451 Acc: 0.6900\n",
      "\n",
      "Epoch 105/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9966\n",
      "val Loss: 0.0464 Acc: 0.6800\n",
      "\n",
      "Epoch 106/249\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9891\n",
      "val Loss: 0.0445 Acc: 0.6800\n",
      "\n",
      "Epoch 107/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9965\n",
      "val Loss: 0.0449 Acc: 0.6800\n",
      "\n",
      "Epoch 108/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9968\n",
      "val Loss: 0.0440 Acc: 0.6800\n",
      "\n",
      "Epoch 109/249\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 0.9926\n",
      "val Loss: 0.0485 Acc: 0.6800\n",
      "\n",
      "Epoch 110/249\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9898\n",
      "val Loss: 0.0484 Acc: 0.6200\n",
      "\n",
      "Epoch 111/249\n",
      "----------\n",
      "train Loss: 0.0050 Acc: 0.9604\n",
      "val Loss: 0.0484 Acc: 0.6800\n",
      "\n",
      "Epoch 112/249\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9917\n",
      "val Loss: 0.0488 Acc: 0.6600\n",
      "\n",
      "Epoch 113/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9953\n",
      "val Loss: 0.0462 Acc: 0.6700\n",
      "\n",
      "Epoch 114/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9968\n",
      "val Loss: 0.0455 Acc: 0.6800\n",
      "\n",
      "Epoch 115/249\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9965\n",
      "val Loss: 0.0455 Acc: 0.6700\n",
      "\n",
      "Epoch 116/249\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9922\n",
      "val Loss: 0.0434 Acc: 0.6800\n",
      "\n",
      "Epoch 117/249\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9966\n",
      "val Loss: 0.0450 Acc: 0.6700\n",
      "\n",
      "Epoch 118/249\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.9912\n",
      "val Loss: 0.0441 Acc: 0.6500\n",
      "\n",
      "Epoch 119/249\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9880\n",
      "val Loss: 0.0516 Acc: 0.6300\n",
      "\n",
      "Epoch 120/249\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9964\n",
      "val Loss: 0.0423 Acc: 0.6700\n",
      "\n",
      "Epoch 121/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9968\n",
      "val Loss: 0.0405 Acc: 0.6900\n",
      "\n",
      "Epoch 122/249\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.9787\n",
      "val Loss: 0.0548 Acc: 0.6700\n",
      "\n",
      "Epoch 123/249\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 0.9917\n",
      "val Loss: 0.0437 Acc: 0.7000\n",
      "\n",
      "Epoch 124/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9968\n",
      "val Loss: 0.0403 Acc: 0.6900\n",
      "\n",
      "Epoch 125/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9968\n",
      "val Loss: 0.0421 Acc: 0.6900\n",
      "\n",
      "Epoch 126/249\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9959\n",
      "val Loss: 0.0420 Acc: 0.7000\n",
      "\n",
      "Epoch 127/249\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 0.9850\n",
      "val Loss: 0.0446 Acc: 0.6900\n",
      "\n",
      "Epoch 128/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9967\n",
      "val Loss: 0.0435 Acc: 0.6800\n",
      "\n",
      "Epoch 129/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9936\n",
      "val Loss: 0.0455 Acc: 0.6900\n",
      "\n",
      "Epoch 130/249\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9928\n",
      "val Loss: 0.0485 Acc: 0.6800\n",
      "\n",
      "Epoch 131/249\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.9903\n",
      "val Loss: 0.0394 Acc: 0.6800\n",
      "\n",
      "Epoch 132/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9964\n",
      "val Loss: 0.0426 Acc: 0.6700\n",
      "\n",
      "Epoch 133/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9943\n",
      "val Loss: 0.0447 Acc: 0.6500\n",
      "\n",
      "Epoch 134/249\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 0.9916\n",
      "val Loss: 0.0421 Acc: 0.6600\n",
      "\n",
      "Epoch 135/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9966\n",
      "val Loss: 0.0441 Acc: 0.6800\n",
      "\n",
      "Epoch 136/249\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9924\n",
      "val Loss: 0.0453 Acc: 0.6500\n",
      "\n",
      "Epoch 137/249\n",
      "----------\n",
      "train Loss: 0.0039 Acc: 0.9714\n",
      "val Loss: 0.0422 Acc: 0.7000\n",
      "\n",
      "Epoch 138/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9969\n",
      "val Loss: 0.0397 Acc: 0.7200\n",
      "\n",
      "Epoch 139/249\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9968\n",
      "val Loss: 0.0423 Acc: 0.7100\n",
      "\n",
      "Epoch 140/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9968\n",
      "val Loss: 0.0404 Acc: 0.7200\n",
      "\n",
      "Epoch 141/249\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9950\n",
      "val Loss: 0.0437 Acc: 0.6900\n",
      "\n",
      "Epoch 142/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9966\n",
      "val Loss: 0.0411 Acc: 0.7000\n",
      "\n",
      "Epoch 143/249\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.9947\n",
      "val Loss: 0.0426 Acc: 0.6700\n",
      "\n",
      "Epoch 144/249\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9968\n",
      "val Loss: 0.0396 Acc: 0.6900\n",
      "\n",
      "Epoch 145/249\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9968\n",
      "val Loss: 0.0409 Acc: 0.7000\n",
      "\n",
      "Epoch 146/249\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9890\n",
      "val Loss: 0.0465 Acc: 0.7000\n",
      "\n",
      "Epoch 147/249\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9907\n",
      "val Loss: 0.0411 Acc: 0.6900\n",
      "\n",
      "Epoch 148/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9969\n",
      "val Loss: 0.0427 Acc: 0.6600\n",
      "\n",
      "Epoch 149/249\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9968\n",
      "val Loss: 0.0401 Acc: 0.6700\n",
      "\n",
      "Epoch 150/249\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 0.9868\n",
      "val Loss: 0.0427 Acc: 0.6700\n",
      "\n",
      "Epoch 151/249\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9970\n",
      "val Loss: 0.0437 Acc: 0.6700\n",
      "\n",
      "Epoch 152/249\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-12895:\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Process Process-12894:\n",
      "Process Process-12893:\n",
      "Process Process-12896:\n",
      "Process Process-12891:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-12892:\n",
      "Process Process-12890:\n",
      "Process Process-12889:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-6550be6a8fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdset_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdset_sizes\u001b[0m\u001b[0;34m,\u001b[0m                                                                         \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m                                                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-0c0e60243c87>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dset_loaders, dset_sizes, criterion, optimizer, lr_scheduler, num_epochs, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdset_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# get the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/roman/miniconda3/envs/ml/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "best_model, train_losses, val_losses, train_accs, val_accs = train_model(model, dset_loaders, dset_sizes,\\\n",
    "                                                                         criterion, optimizer, num_epochs=250,\\\n",
    "                                                                         verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(train_losses, val_losses, train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,_ = k_fold_cv(model, train_input, train_target, criterion, optimizer, \n",
    "                    num_epochs=250, K=5, shuffle=True, random_seed=42, verbose=1,\n",
    "                    augment_multiplier=0, std_dev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_var = Variable(train_input.cuda())\n",
    "train_target_var = Variable(train_target.cuda(), requires_grad=False)\n",
    "\n",
    "test_input_var = Variable(test_input.cuda())\n",
    "test_target_var = Variable(test_target.cuda(), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = best_model(test_input_var).data\n",
    "pred_test_classes = pred_test.max(1)[1]\n",
    "accuracy_score(pred_test_classes, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = best_model(train_input_var).data\n",
    "pred_train_classes = pred_train.max(1)[1]\n",
    "accuracy_score(pred_train_classes, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
